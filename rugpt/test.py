import json
import torch
import pickle
import os
import re
from typing import List, Dict
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
from sentence_transformers import SentenceTransformer
import faiss
import evaluate
from tqdm import tqdm

class MinimalRAG:
    def __init__(self, use_cpu: bool = False):
        print("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è RAG –º–æ–¥–µ–ª–µ–π...")
        
        self.embedder = SentenceTransformer("cointegrated/rubert-tiny2")
        model_name = 'rugpt3-koap-finetuned'
        
        print(f"–ó–∞–≥—Ä—É–∑–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–π –º–æ–¥–µ–ª–∏: {model_name}")
        
        if torch.cuda.is_available() and not use_cpu:
            device = "cuda:0"
            torch_dtype = torch.float16
            print("–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è GPU –¥–ª—è RAG")
        else:
            device = "cpu"
            torch_dtype = torch.float32
            print("–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CPU –¥–ª—è RAG")
        
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(model_name)
            self.tokenizer.pad_token = self.tokenizer.eos_token
            
            self.model = AutoModelForCausalLM.from_pretrained(
                model_name,
                torch_dtype=torch_dtype,
                low_cpu_mem_usage=True,
                trust_remote_code=True
            )
            
            self.model.to(device)
            
            self.generator = pipeline(
                "text-generation",
                model=self.model,
                tokenizer=self.tokenizer,
                device=0 if device == "cuda:0" else -1
            )
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ {model_name}: {e}")
            print("–ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∏—Ç—å –±–æ–ª–µ–µ –ª–µ–≥–∫—É—é –º–æ–¥–µ–ª—å...")
            assert(False)
            model_name = "sberbank-ai/rugpt3small_based_on_gpt2"
            self.tokenizer = AutoTokenizer.from_pretrained(model_name)
            self.tokenizer.pad_token = self.tokenizer.eos_token
            self.model = AutoModelForCausalLM.from_pretrained(model_name)
            self.model.to(device)
            self.generator = pipeline(
                "text-generation",
                model=self.model,
                tokenizer=self.tokenizer,
                device=0 if device == "cuda:0" else -1
            )
        
        self.index = None
        self.chunks = []
        self.chunk_info = []
        print("RAG –º–æ–¥–µ–ª–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ!")
    
    def load_text_file(self, filepath: str, chunk_size: int = 300):
        print(f"–ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–∞: {filepath}")
        with open(filepath, 'r', encoding='windows-1251') as f:
            text = f.read()
        
        sentences = re.split(r'(?<=[.!?])\s+|(?<=\n)\s*', text)
        
        current_chunk = ""
        current_article = "–ù–µ —É–∫–∞–∑–∞–Ω–æ"
        
        for sentence in sentences:
            if "–°—Ç–∞—Ç—å—è" in sentence[:50] or "—Å—Ç." in sentence[:50]:
                current_article = sentence[:100]
            
            if len(current_chunk) + len(sentence) < chunk_size:
                current_chunk += sentence + " "
            else:
                if current_chunk:
                    self.chunks.append(current_chunk.strip())
                    self.chunk_info.append({
                        "source": filepath,
                        "article": current_article,
                        "length": len(current_chunk)
                    })
                current_chunk = sentence + " "
        
        if current_chunk:
            self.chunks.append(current_chunk.strip())
            self.chunk_info.append({
                "source": filepath,
                "article": current_article,
                "length": len(current_chunk)
            })
        
        print(f"–°–æ–∑–¥–∞–Ω–æ {len(self.chunks)} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤")
    
    def build_index(self):
        if not self.chunks:
            print("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏. –°–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª—ã.")
            return
        
        print(f"–°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è {len(self.chunks)} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤...")
        embeddings = self.embedder.encode(
            self.chunks, 
            show_progress_bar=True,
            batch_size=32,
            convert_to_numpy=True
        )
        
        dim = embeddings.shape[1]
        self.index = faiss.IndexFlatL2(dim)
        self.index.add(embeddings.astype('float32'))
        print(f"–ò–Ω–¥–µ–∫—Å –ø–æ—Å—Ç—Ä–æ–µ–Ω. –í–µ–∫—Ç–æ—Ä–æ–≤: {self.index.ntotal}")
    
    def save_index(self, filename: str = "rag_index.pkl"):
        data = {
            'chunks': self.chunks,
            'chunk_info': self.chunk_info,
            'index': faiss.serialize_index(self.index) if self.index else None
        }
        
        with open(filename, "wb") as f:
            pickle.dump(data, f)
        
        print(f"–ò–Ω–¥–µ–∫—Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {filename}")
    
    def load_index(self, filename: str = "rag_index.pkl"):
        if not os.path.exists(filename):
            print(f"–§–∞–π–ª {filename} –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return False
        
        with open(filename, "rb") as f:
            data = pickle.load(f)
            self.chunks = data['chunks']
            self.chunk_info = data['chunk_info']
            if 'index' in data.keys():
                self.index = faiss.deserialize_index(data['index'])
        
        print(f"–ò–Ω–¥–µ–∫—Å –∑–∞–≥—Ä—É–∂–µ–Ω –∏–∑ {filename}")
        print(f"–§—Ä–∞–≥–º–µ–Ω—Ç–æ–≤: {len(self.chunks)}")
        return True
    
    def search(self, query: str, top_k: int = 3) -> List[Dict]:
        if self.index is None:
            print("–ò–Ω–¥–µ–∫—Å –Ω–µ –ø–æ—Å—Ç—Ä–æ–µ–Ω. –°–Ω–∞—á–∞–ª–∞ —Å–æ–∑–¥–∞–π—Ç–µ –∏–Ω–¥–µ–∫—Å.")
            return []
        
        query_embedding = self.embedder.encode([query])
        distances, indices = self.index.search(query_embedding.astype('float32'), top_k)
        
        results = []
        for i, idx in enumerate(indices[0]):
            if idx < len(self.chunks):
                results.append({
                    'text': self.chunks[idx],
                    'distance': float(distances[0][i]),
                    'info': self.chunk_info[idx]
                })
        
        return results
    
    def answer(self, question: str, top_k_context: int = 3) -> str:
        if not self.chunks:
            return "–°–∏—Å—Ç–µ–º–∞ –Ω–µ –≥–æ—Ç–æ–≤–∞. –°–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∑–∏—Ç–µ –¥–∞–Ω–Ω—ã–µ –ö–æ–ê–ü."
        
        context_chunks = self.search(question, top_k=top_k_context)
        
        if not context_chunks:
            return "–ù–µ –Ω–∞–π–¥–µ–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –ö–æ–ê–ü –†–§."
        
        context = "\n".join([f"–§—Ä–∞–≥–º–µ–Ω—Ç {i+1} (—Å—Ç–∞—Ç—å—è: {chunk['info'].get('article', '–ù–µ —É–∫–∞–∑–∞–Ω–∞')}):\n{chunk['text']}" 
                           for i, chunk in enumerate(context_chunks)])
        
        prompt = f"""–¢—ã - —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç, —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä—É—é—â–∏–π—Å—è –Ω–∞ –ö–æ–¥–µ–∫—Å–µ –æ–± –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–∏–≤–Ω—ã—Ö –ø—Ä–∞–≤–æ–Ω–∞—Ä—É—à–µ–Ω–∏—è—Ö –†–§ (–ö–æ–ê–ü –†–§).

–¢–µ–±–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω—ã —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –∏–∑ –ö–æ–ê–ü –†–§:

{context}

–ù–∞ –æ—Å–Ω–æ–≤–∞–Ω–∏–∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –ö–æ–ê–ü –†–§ –æ—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å: {question}

–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ –æ—Ç–≤–µ—Ç—É:
1. –ë—É–¥—å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —Ç–æ—á–Ω—ã–º –∏ –∏—Å–ø–æ–ª—å–∑—É–π —Ç–æ–ª—å–∫–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤
2. –£–∫–∞–∂–∏, –µ—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ–ø–æ–ª–Ω–∞—è
3. –ï—Å–ª–∏ –æ—Ç–≤–µ—Ç–∞ –Ω–µ—Ç –≤ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞—Ö, —Ç–∞–∫ –∏ —Å–∫–∞–∂–∏
4. –§–æ—Ä–º—É–ª–∏—Ä—É–π –æ—Ç–≤–µ—Ç –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–º —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–º —è–∑—ã–∫–æ–º

–û—Ç–≤–µ—Ç:"""
        
        try:
            result = self.generator(
                prompt,
                max_new_tokens=400,
                temperature=0.7,
                do_sample=True,
                top_p=0.9,
                repetition_penalty=1.1,
                pad_token_id=self.tokenizer.eos_token_id,
                num_return_sequences=1
            )
            
            generated_text = result[0]['generated_text']
            
            if "–û—Ç–≤–µ—Ç:" in generated_text:
                answer = generated_text.split("–û—Ç–≤–µ—Ç:")[-1].strip()
            else:
                answer = generated_text.strip().split('\n')[-1].strip()
            
            return answer
            
        except Exception as e:
            return f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞: {str(e)}"

def calculate_rouge(predictions, references):
    rouge = evaluate.load('rouge')
    results = rouge.compute(
        predictions=predictions,
        references=references,
        use_stemmer=True
    )
    return results

def calculate_bleu(predictions, references):
    bleu = evaluate.load('bleu')
    results = bleu.compute(
        predictions=predictions,
        references=references
    )
    return results

def calculate_exact_match(predictions, references):
    exact_match = evaluate.load("exact_match")
    results = exact_match.compute(
        predictions=predictions,
        references=references,
        ignore_case=True,
        ignore_punctuation=True
    )
    return results

def load_test_data(filepath: str = "data/test_data.json"):
    if not os.path.exists(filepath):
        print(f"–§–∞–π–ª {filepath} –Ω–µ –Ω–∞–π–¥–µ–Ω")
        return []
    
    with open(filepath, 'r', encoding='utf-8') as f:
        test_data = json.load(f)
    return test_data

def load_koap_data(rag_system: MinimalRAG, data_dir: str = "data/koap_data"):
    if not os.path.exists(data_dir):
        print(f"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è {data_dir} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞.")
        return False
    
    txt_files = [f for f in os.listdir(data_dir) if f.endswith('.txt')]
    
    if not txt_files:
        print(f"–ù–µ –Ω–∞–π–¥–µ–Ω–æ .txt —Ñ–∞–π–ª–æ–≤ –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ {data_dir}")
        return False
    
    for txt_file in txt_files:
        filepath = os.path.join(data_dir, txt_file)
        rag_system.load_text_file(filepath)
    
    return True

def evaluate_with_rag(model_path: str = "./rugpt3-koap-finetuned", 
                     use_rag: bool = True,
                     rag_index_path: str = "rag_index.pkl",
                     koap_data_dir: str = "data/koap_data"):
    print("="*70)
    print(f"–û–¶–ï–ù–ö–ê –ú–û–î–ï–õ–ò {'–° RAG' if use_rag else '–ë–ï–ó RAG'}")
    print(f"–ú–æ–¥–µ–ª—å: {model_path}")
    print("="*70)
    
    rag_system = None
    if use_rag:
        print("\nüîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è RAG —Å–∏—Å—Ç–µ–º—ã...")
        rag_system = MinimalRAG(use_cpu=True)
        
        if os.path.exists(rag_index_path):
            print(f"–ó–∞–≥—Ä—É–∑–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞: {rag_index_path}")
            rag_system.load_index(rag_index_path)
        else:
            print(f"–ò–Ω–¥–µ–∫—Å {rag_index_path} –Ω–µ –Ω–∞–π–¥–µ–Ω. –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–≥–æ...")
            if load_koap_data(rag_system, koap_data_dir):
                rag_system.build_index()
                rag_system.save_index(rag_index_path)
            else:
                print("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –ö–æ–ê–ü. –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –Ω–∞ —Ä–µ–∂–∏–º –±–µ–∑ RAG.")
                use_rag = False
    
    print("\nüìö –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
    test_data = load_test_data()
    if not test_data:
        print("–¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã!")
        return
    
    print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(test_data)} —Ç–µ—Å—Ç–æ–≤—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤")
    
    predictions_with_rag = []
    predictions_without_rag = []
    references = []
    questions = []
    
    print("\nü§ñ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–æ–≤...")
    
    if not use_rag or True:
        print("–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –±–µ–∑ RAG...")
        tokenizer = AutoTokenizer.from_pretrained(model_path)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        device = 0 if torch.cuda.is_available() else -1
        model = AutoModelForCausalLM.from_pretrained(model_path)
        generator_without_rag = pipeline(
            "text-generation",
            model=model,
            tokenizer=tokenizer,
            device=device
        )
    
    for item in tqdm(test_data, desc="–û–±—Ä–∞–±–æ—Ç–∫–∞ –≤–æ–ø—Ä–æ—Å–æ–≤"):
        question = item["instruction"]
        reference = item["output"]
        
        questions.append(question)
        references.append(reference)
        
        if use_rag and rag_system:
            try:
                answer_with_rag = rag_system.answer(question)
                predictions_with_rag.append(answer_with_rag)
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å RAG: {e}")
                predictions_with_rag.append("–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏")
        else:
            predictions_with_rag.append("RAG –Ω–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª—Å—è")
        
        try:
            prompt = f"–í–æ–ø—Ä–æ—Å: {question}\n–û—Ç–≤–µ—Ç:"
            
            result = generator_without_rag(
                prompt,
                max_new_tokens=200,
                temperature=0.7,
                do_sample=True,
                top_p=0.9,
                repetition_penalty=1.1,
                pad_token_id=tokenizer.eos_token_id
            )
            
            full_text = result[0]['generated_text']
            if "–û—Ç–≤–µ—Ç:" in full_text:
                answer_without_rag = full_text.split("–û—Ç–≤–µ—Ç:")[-1].strip()
            else:
                answer_without_rag = full_text
            
            predictions_without_rag.append(answer_without_rag)
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –±–µ–∑ RAG: {e}")
            predictions_without_rag.append("–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏")
    
    print("\n" + "="*70)
    print("–í–´–ß–ò–°–õ–ï–ù–ò–ï –ú–ï–¢–†–ò–ö")
    print("="*70)
    
    results = {
        "model_path": model_path,
        "use_rag": use_rag,
        "test_samples": len(test_data),
        "questions": questions,
        "references": references
    }
    
    if use_rag:
        print("\nüìä –ú–ï–¢–†–ò–ö–ò –° –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–ï–ú RAG:")
        print("-" * 40)
        
        rouge_scores_rag = calculate_rouge(predictions_with_rag, references)
        print("ROUGE –º–µ—Ç—Ä–∏–∫–∏:")
        for key, value in rouge_scores_rag.items():
            if isinstance(value, float):
                print(f"  {key}: {value:.4f}")
        
        bleu_scores_rag = calculate_bleu(predictions_with_rag, references)
        print(f"\nBLEU –º–µ—Ç—Ä–∏–∫–∞: {bleu_scores_rag['bleu']:.4f}")
        
        exact_match_rag = calculate_exact_match(predictions_with_rag, references)
        print(f"Exact Match: {exact_match_rag['exact_match']:.4f}")
        
        results["metrics_with_rag"] = {
            "rouge": rouge_scores_rag,
            "bleu": bleu_scores_rag,
            "exact_match": exact_match_rag,
            "predictions": predictions_with_rag
        }
    
    print("\nüìä –ú–ï–¢–†–ò–ö–ò –ë–ï–ó RAG:")
    print("-" * 40)
    
    rouge_scores_no_rag = calculate_rouge(predictions_without_rag, references)
    print("ROUGE –º–µ—Ç—Ä–∏–∫–∏:")
    for key, value in rouge_scores_no_rag.items():
        if isinstance(value, float):
            print(f"  {key}: {value:.4f}")
    
    bleu_scores_no_rag = calculate_bleu(predictions_without_rag, references)
    print(f"\nBLEU –º–µ—Ç—Ä–∏–∫–∞: {bleu_scores_no_rag['bleu']:.4f}")
    
    exact_match_no_rag = calculate_exact_match(predictions_without_rag, references)
    print(f"Exact Match: {exact_match_no_rag['exact_match']:.4f}")
    
    results["metrics_without_rag"] = {
        "rouge": rouge_scores_no_rag,
        "bleu": bleu_scores_no_rag,
        "exact_match": exact_match_no_rag,
        "predictions": predictions_without_rag
    }
    
    print("\n" + "="*70)
    print("–°–†–ê–í–ù–ò–¢–ï–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó")
    print("="*70)
    
    if use_rag:
        print("\nüìà –£–õ–£–ß–®–ï–ù–ò–ï/–£–•–£–î–®–ï–ù–ò–ï –ü–†–ò –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–ò RAG:")
        print("-" * 40)
        
        metrics_to_compare = ['rouge1', 'rouge2', 'rougeL', 'bleu', 'exact_match']
        for metric in metrics_to_compare:
            if metric == 'bleu':
                rag_val = bleu_scores_rag['bleu']
                no_rag_val = bleu_scores_no_rag['bleu']
            elif metric == 'exact_match':
                rag_val = exact_match_rag['exact_match']
                no_rag_val = exact_match_no_rag['exact_match']
            else:
                rag_val = rouge_scores_rag[metric]
                no_rag_val = rouge_scores_no_rag[metric]
            
            improvement = ((rag_val - no_rag_val) / no_rag_val * 100) if no_rag_val > 0 else 0
            trend = "üîº –£–õ–£–ß–®–ï–ù–ò–ï" if improvement > 0 else "üîΩ –£–•–£–î–®–ï–ù–ò–ï"
            
            print(f"{metric.upper():12} | –ë–µ–∑ RAG: {no_rag_val:.4f} | –° RAG: {rag_val:.4f} | {trend}: {improvement:+.2f}%")
    
    print("\nüìè –ê–ù–ê–õ–ò–ó –î–õ–ò–ù –û–¢–í–ï–¢–û–í:")
    print("-" * 40)
    
    if use_rag:
        avg_len_rag = sum(len(p.split()) for p in predictions_with_rag if isinstance(p, str)) / len(predictions_with_rag)
        print(f"–°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ –æ—Ç–≤–µ—Ç–∞ —Å RAG: {avg_len_rag:.1f} —Å–ª–æ–≤")
    
    avg_len_no_rag = sum(len(p.split()) for p in predictions_without_rag if isinstance(p, str)) / len(predictions_without_rag)
    print(f"–°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ –æ—Ç–≤–µ—Ç–∞ –±–µ–∑ RAG: {avg_len_no_rag:.1f} —Å–ª–æ–≤")
    
    avg_len_ref = sum(len(r.split()) for r in references) / len(references)
    print(f"–°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —ç—Ç–∞–ª–æ–Ω–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞: {avg_len_ref:.1f} —Å–ª–æ–≤")
    
    print("\n" + "="*70)
    print("–ü–†–ò–ú–ï–†–´ –û–¢–í–ï–¢–û–í (–ø–µ—Ä–≤—ã–µ 3 –≤–æ–ø—Ä–æ—Å–∞)")
    print("="*70)
    
    for i in range(min(3, len(questions))):
        print(f"\n{i+1}. –í–û–ü–†–û–°: {questions[i]}")
        print(f"   –≠–¢–ê–õ–û–ù: {references[i]}")
        
        if use_rag:
            print(f"   –° RAG: {predictions_with_rag[i][:150]}...")
        
        print(f"   –ë–ï–ó RAG: {predictions_without_rag[i][:150]}...")
        print("-" * 50)
    
    output_file = f"evaluation_results_{'with_rag' if use_rag else 'without_rag'}_{len(test_data)}_samples.json"
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f"\n‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {output_file}")
    
    return results

def quick_test_with_rag():
    print("üöÄ –ë–´–°–¢–†–´–ô –¢–ï–°–¢ RAG –°–ò–°–¢–ï–ú–´")
    
    rag = MinimalRAG(use_cpu=True)
    
    if not rag.load_index("rag_index.pkl"):
        print("–ò–Ω–¥–µ–∫—Å –Ω–µ –Ω–∞–π–¥–µ–Ω. –ó–∞–ø—É—Å—Ç–∏—Ç–µ –ø–æ–ª–Ω—É—é –æ—Ü–µ–Ω–∫—É —Å–Ω–∞—á–∞–ª–∞.")
        return
    
    test_questions = [
        "–ö–∞–∫–∞—è —Å—Ç–∞—Ç—å—è –ö–æ–ê–ü –†–§ —Ä–µ–≥—É–ª–∏—Ä—É–µ—Ç –ø—Ä–µ–≤—ã—à–µ–Ω–∏–µ —Å–∫–æ—Ä–æ—Å—Ç–∏?",
        "–ö–∞–∫–æ–π —à—Ç—Ä–∞—Ñ –∑–∞ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–º –±–µ–∑ –ø—Ä–∞–≤?",
        "–ß—Ç–æ —Ç–∞–∫–æ–µ –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–∏–≤–Ω–æ–µ –ø—Ä–∞–≤–æ–Ω–∞—Ä—É—à–µ–Ω–∏–µ?"
    ]
    
    print("\n" + "="*60)
    for i, question in enumerate(test_questions, 1):
        print(f"\n{i}. –í–û–ü–†–û–°: {question}")
        print("-" * 40)
        
        context_chunks = rag.search(question, top_k=2)
        if context_chunks:
            print("–ù–∞–π–¥–µ–Ω–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã:")
            for j, chunk in enumerate(context_chunks, 1):
                article = chunk['info'].get('article', '–ù–µ —É–∫–∞–∑–∞–Ω–∞')[:50]
                print(f"  {j}. {article}")
        
        answer = rag.answer(question)
        print(f"\n–û–¢–í–ï–¢: {answer}")

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='–û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ —Å RAG —Å–∏—Å—Ç–µ–º–æ–π')
    parser.add_argument('--model_path', type=str, default="./rugpt3-koap-finetuned",
                       help='–ü—É—Ç—å –∫ –º–æ–¥–µ–ª–∏')
    parser.add_argument('--no_rag', action='store_true',
                       help='–û—Ü–µ–Ω–∏–≤–∞—Ç—å —Ç–æ–ª—å–∫–æ –±–µ–∑ RAG')
    parser.add_argument('--quick_test', action='store_true',
                       help='–ë—ã—Å—Ç—Ä—ã–π —Ç–µ—Å—Ç RAG —Å–∏—Å—Ç–µ–º—ã')
    parser.add_argument('--rag_index', type=str, default="rag_index.pkl",
                       help='–ü—É—Ç—å –∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–º—É –∏–Ω–¥–µ–∫—Å—É RAG')
    parser.add_argument('--koap_data', type=str, default="data/koap_data",
                       help='–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –¥–∞–Ω–Ω—ã–º–∏ –ö–æ–ê–ü')
    
    args = parser.parse_args()
    
    if args.quick_test:
        quick_test_with_rag()
    else:
        evaluate_with_rag(
            model_path=args.model_path,
            use_rag=not args.no_rag,
            rag_index_path=args.rag_index,
            koap_data_dir=args.koap_data
        )