import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
import json
import pickle
import os
import re
from collections import Counter
from typing import List, Dict
from tqdm import tqdm
import evaluate
from sentence_transformers import SentenceTransformer
import faiss

class MinimalRAG:
    def __init__(self, use_cpu: bool = False):
        print("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è RAG –º–æ–¥–µ–ª–µ–π...")
        
        self.embedder = SentenceTransformer("cointegrated/rubert-tiny2")
        
        self.index = None
        self.chunks = []
        self.chunk_info = []
        print("RAG –º–æ–¥–µ–ª—å –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")
    
    def load_text_file(self, filepath: str, chunk_size: int = 300):
        print(f"–ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–∞: {filepath}")
        with open(filepath, 'r', encoding='windows-1251') as f:
            text = f.read()
        
        sentences = re.split(r'(?<=[.!?])\s+|(?<=\n)\s*', text)
        
        current_chunk = ""
        current_article = "–ù–µ —É–∫–∞–∑–∞–Ω–æ"
        
        for sentence in sentences:
            if "–°—Ç–∞—Ç—å—è" in sentence[:50] or "—Å—Ç." in sentence[:50]:
                current_article = sentence[:100]
            
            if len(current_chunk) + len(sentence) < chunk_size:
                current_chunk += sentence + " "
            else:
                if current_chunk:
                    self.chunks.append(current_chunk.strip())
                    self.chunk_info.append({
                        "source": filepath,
                        "article": current_article,
                        "length": len(current_chunk)
                    })
                current_chunk = sentence + " "
        
        if current_chunk:
            self.chunks.append(current_chunk.strip())
            self.chunk_info.append({
                "source": filepath,
                "article": current_article,
                "length": len(current_chunk)
            })
        
        print(f"–°–æ–∑–¥–∞–Ω–æ {len(self.chunks)} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤")
    
    def build_index(self):
        if not self.chunks:
            print("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏. –°–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª—ã.")
            return
        
        print(f"–°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è {len(self.chunks)} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤...")
        embeddings = self.embedder.encode(
            self.chunks, 
            show_progress_bar=True,
            batch_size=32,
            convert_to_numpy=True
        )
        
        dim = embeddings.shape[1]
        self.index = faiss.IndexFlatL2(dim)
        self.index.add(embeddings.astype('float32'))
        print(f"–ò–Ω–¥–µ–∫—Å –ø–æ—Å—Ç—Ä–æ–µ–Ω. –í–µ–∫—Ç–æ—Ä–æ–≤: {self.index.ntotal}")
    
    def save_index(self, filename: str = "rag_index.pkl"):
        data = {
            'chunks': self.chunks,
            'chunk_info': self.chunk_info,
            'index': faiss.serialize_index(self.index) if self.index else None
        }
        
        with open(filename, "wb") as f:
            pickle.dump(data, f)
        
        print(f"–ò–Ω–¥–µ–∫—Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {filename}")
    
    def load_index(self, filename: str = "rag_index.pkl"):
        if not os.path.exists(filename):
            print(f"–§–∞–π–ª {filename} –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return False
        
        with open(filename, "rb") as f:
            data = pickle.load(f)
            self.chunks = data['chunks']
            self.chunk_info = data['chunk_info']
            if 'index' in data.keys():
                self.index = faiss.deserialize_index(data['index'])
        
        print(f"–ò–Ω–¥–µ–∫—Å –∑–∞–≥—Ä—É–∂–µ–Ω –∏–∑ {filename}")
        print(f"–§—Ä–∞–≥–º–µ–Ω—Ç–æ–≤: {len(self.chunks)}")
        return True
    
    def search(self, query: str, top_k: int = 3) -> List[Dict]:
        if self.index is None:
            print("–ò–Ω–¥–µ–∫—Å –Ω–µ –ø–æ—Å—Ç—Ä–æ–µ–Ω. –°–Ω–∞—á–∞–ª–∞ —Å–æ–∑–¥–∞–π—Ç–µ –∏–Ω–¥–µ–∫—Å.")
            return []
        
        query_embedding = self.embedder.encode([query])
        distances, indices = self.index.search(query_embedding.astype('float32'), top_k)
        
        results = []
        for i, idx in enumerate(indices[0]):
            if idx < len(self.chunks):
                results.append({
                    'text': self.chunks[idx],
                    'distance': float(distances[0][i]),
                    'info': self.chunk_info[idx]
                })
        
        return results

class Vocabulary:
    def __init__(self):
        self.word2idx = {"<PAD>": 0, "<UNK>": 1, "<SOS>": 2, "<EOS>": 3}
        self.idx2word = {0: "<PAD>", 1: "<UNK>", 2: "<SOS>", 3: "<EOS>"}
        self.word_count = {}
        
    def build_vocab(self, texts, min_freq=1):
        counter = Counter()
        for text in texts:
            words = text.split()
            counter.update(words)
        
        idx = len(self.word2idx)
        for word, count in counter.items():
            if count >= min_freq:
                self.word2idx[word] = idx
                self.idx2word[idx] = word
                self.word_count[word] = count
                idx += 1
                
        print(f"–†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è: {len(self.word2idx)} —Å–ª–æ–≤")
        return len(self.word2idx)
    
    def text_to_sequence(self, text):
        words = text.split()
        sequence = [self.word2idx.get(word, self.word2idx["<UNK>"]) for word in words]
        return sequence
    
    def sequence_to_text(self, sequence):
        words = [self.idx2word.get(idx, "<UNK>") for idx in sequence]
        return " ".join(words)

class LSTMLanguageModel(nn.Module):
    def __init__(self, vocab_size, embedding_dim=128, hidden_dim=256, num_layers=2, dropout=0.3):
        super().__init__()
        
        self.vocab_size = vocab_size
        self.embedding_dim = embedding_dim
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers
        
        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)
        
        self.lstm = nn.LSTM(
            input_size=embedding_dim,
            hidden_size=hidden_dim,
            num_layers=num_layers,
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0,
            bidirectional=False
        )
        
        self.fc = nn.Linear(hidden_dim, vocab_size)
        
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, x, hidden=None):
        batch_size, seq_len = x.size()
        
        embedded = self.embedding(x)
        embedded = self.dropout(embedded)
        
        lstm_out, hidden = self.lstm(embedded, hidden)
        lstm_out = self.dropout(lstm_out)
        
        output = self.fc(lstm_out)
        
        return output, hidden
    
    def init_hidden(self, batch_size, device='cpu'):
        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)
        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)
        return (h0, c0)
    
    def generate(self, start_text, vocab, max_len=50, temperature=1.0, device='cpu'):
        self.eval()
        
        words = start_text.split()
        sequence = [vocab.word2idx["<SOS>"]] + [vocab.word2idx.get(word, vocab.word2idx["<UNK>"]) for word in words]
        sequence_tensor = torch.tensor(sequence, dtype=torch.long).unsqueeze(0).to(device)
        
        generated = sequence.copy()
        hidden = None
        
        with torch.no_grad():
            for _ in range(max_len):
                output, hidden = self(sequence_tensor, hidden)
                
                last_logits = output[:, -1, :] / temperature
                
                probs = torch.softmax(last_logits, dim=-1)
                
                next_token = torch.multinomial(probs, 1).item()
                
                if next_token == vocab.word2idx["<EOS>"]:
                    break
                
                generated.append(next_token)
                
                sequence_tensor = torch.tensor([[next_token]], dtype=torch.long).to(device)
        
        generated_text = vocab.sequence_to_text(generated[1:])
        return generated_text

def calculate_rouge(predictions, references):
    rouge = evaluate.load('rouge')
    results = rouge.compute(
        predictions=predictions,
        references=references,
        use_stemmer=True
    )
    return results

def calculate_bleu(predictions, references):
    bleu = evaluate.load('bleu')
    results = bleu.compute(
        predictions=predictions,
        references=references
    )
    return results

def calculate_exact_match(predictions, references):
    exact_match = evaluate.load("exact_match")
    results = exact_match.compute(
        predictions=predictions,
        references=references,
        ignore_case=True,
        ignore_punctuation=True
    )
    return results

def load_lstm_model(model_path="lstm_koap_model_best.pt", vocab_path="lstm_koap_model_vocab.pkl"):
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")
    
    with open(vocab_path, "rb") as f:
        vocab = pickle.load(f)
    
    checkpoint = torch.load(model_path, map_location=device)
    
    model = LSTMLanguageModel(
        vocab_size=checkpoint['vocab_size'],
        embedding_dim=checkpoint['embedding_dim'],
        hidden_dim=checkpoint['hidden_dim'],
        num_layers=checkpoint['num_layers'],
        dropout=checkpoint.get('dropout', 0.3)
    ).to(device)
    
    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()
    
    print(f"LSTM –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ {model_path}")
    print(f"–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: vocab_size={checkpoint['vocab_size']}, "
          f"hidden_dim={checkpoint['hidden_dim']}, "
          f"layers={checkpoint['num_layers']}")
    
    return model, vocab, device

def load_test_data(filepath: str = "data/test_data.json"):
    if not os.path.exists(filepath):
        print(f"–§–∞–π–ª {filepath} –Ω–µ –Ω–∞–π–¥–µ–Ω")
        return []
    
    with open(filepath, 'r', encoding='utf-8') as f:
        test_data = json.load(f)
    return test_data

def load_koap_data(rag_system: MinimalRAG, data_dir: str = "data/koap_data"):
    if not os.path.exists(data_dir):
        print(f"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è {data_dir} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞.")
        return False
    
    txt_files = [f for f in os.listdir(data_dir) if f.endswith('.txt')]
    
    if not txt_files:
        print(f"–ù–µ –Ω–∞–π–¥–µ–Ω–æ .txt —Ñ–∞–π–ª–æ–≤ –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ {data_dir}")
        return False
    
    for txt_file in txt_files:
        filepath = os.path.join(data_dir, txt_file)
        rag_system.load_text_file(filepath)
    
    return True

def extract_answer_from_generated(text, question):
    if f"–í–æ–ø—Ä–æ—Å: {question}" in text:
        text = text.replace(f"–í–æ–ø—Ä–æ—Å: {question}", "")
    
    if "–û—Ç–≤–µ—Ç:" in text:
        answer = text.split("–û—Ç–≤–µ—Ç:")[-1].strip()
    else:
        sentences = re.split(r'[.!?]+', text)
        if len(sentences) > 1:
            answer = sentences[-2].strip() if sentences[-1].strip() == "" else sentences[-1].strip()
        else:
            answer = text.strip()
    
    return answer

def generate_with_lstm(model, vocab, prompt, max_len=100, temperature=0.7, device='cpu'):
    generated_text = model.generate(prompt, vocab, max_len=max_len, temperature=temperature, device=device)
    return generated_text

def evaluate_lstm_with_rag(
    model_path: str = "lstm_koap_model_best.pt",
    vocab_path: str = "lstm_koap_model_vocab.pkl",
    use_rag: bool = True,
    rag_index_path: str = "rag_index.pkl",
    koap_data_dir: str = "data/koap_data",
    test_data_path: str = "data/test_data.json",
    max_gen_len: int = 100,
    temperature: float = 0.7
):
    print("="*70)
    print(f"–û–¶–ï–ù–ö–ê LSTM –ú–û–î–ï–õ–ò {'–° RAG' if use_rag else '–ë–ï–ó RAG'}")
    print(f"–ú–æ–¥–µ–ª—å: {model_path}")
    print("="*70)
    
    rag_system = None
    if use_rag:
        print("\nüîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è RAG —Å–∏—Å—Ç–µ–º—ã...")
        rag_system = MinimalRAG(use_cpu=True)
        
        if os.path.exists(rag_index_path):
            print(f"–ó–∞–≥—Ä—É–∑–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞: {rag_index_path}")
            rag_system.load_index(rag_index_path)
        else:
            print(f"–ò–Ω–¥–µ–∫—Å {rag_index_path} –Ω–µ –Ω–∞–π–¥–µ–Ω. –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–≥–æ...")
            if load_koap_data(rag_system, koap_data_dir):
                rag_system.build_index()
                rag_system.save_index(rag_index_path)
            else:
                print("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –ö–æ–ê–ü. –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –Ω–∞ —Ä–µ–∂–∏–º –±–µ–∑ RAG.")
                use_rag = False
    
    print("\nü§ñ –ó–∞–≥—Ä—É–∑–∫–∞ LSTM –º–æ–¥–µ–ª–∏...")
    model, vocab, device = load_lstm_model(model_path, vocab_path)
    
    print("\nüìö –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
    test_data = load_test_data(test_data_path)
    if not test_data:
        print("–¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã!")
        return
    
    print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(test_data)} —Ç–µ—Å—Ç–æ–≤—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤")
    
    predictions_with_rag = []
    predictions_without_rag = []
    references = []
    questions = []
    
    print("\nüß† –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–æ–≤...")
    
    for item in tqdm(test_data, desc="–û–±—Ä–∞–±–æ—Ç–∫–∞ –≤–æ–ø—Ä–æ—Å–æ–≤"):
        question = item["instruction"]
        reference = item["output"]
        
        questions.append(question)
        references.append(reference)
        
        base_prompt = f"–í–æ–ø—Ä–æ—Å: {question} –û—Ç–≤–µ—Ç:"
        
        if use_rag and rag_system:
            try:
                context_chunks = rag_system.search(question, top_k=3)
                
                if context_chunks:
                    context = "\n".join([f"–§—Ä–∞–≥–º–µ–Ω—Ç {i+1} (—Å—Ç–∞—Ç—å—è: {chunk['info'].get('article', '–ù–µ —É–∫–∞–∑–∞–Ω–∞')}):\n{chunk['text']}" 
                                       for i, chunk in enumerate(context_chunks)])
                    
                    rag_prompt = f"""–¢—ã - —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç, —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä—É—é—â–∏–π—Å—è –Ω–∞ –ö–æ–¥–µ–∫—Å–µ –æ–± –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–∏–≤–Ω—ã—Ö –ø—Ä–∞–≤–æ–Ω–∞—Ä—É—à–µ–Ω–∏—è—Ö –†–§ (–ö–æ–ê–ü –†–§).

–¢–µ–±–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω—ã —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –∏–∑ –ö–æ–ê–ü –†–§:

{context}

–ù–∞ –æ—Å–Ω–æ–≤–∞–Ω–∏–∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –ö–æ–ê–ü –†–§ –æ—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å: {question}

–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ –æ—Ç–≤–µ—Ç—É:
1. –ë—É–¥—å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —Ç–æ—á–Ω—ã–º –∏ –∏—Å–ø–æ–ª—å–∑—É–π —Ç–æ–ª—å–∫–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤
2. –£–∫–∞–∂–∏, –µ—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ–ø–æ–ª–Ω–∞—è
3. –ï—Å–ª–∏ –æ—Ç–≤–µ—Ç–∞ –Ω–µ—Ç –≤ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞—Ö, —Ç–∞–∫ –∏ —Å–∫–∞–∂–∏
4. –§–æ—Ä–º—É–ª–∏—Ä—É–π –æ—Ç–≤–µ—Ç –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–º —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–º —è–∑—ã–∫–æ–º

–û—Ç–≤–µ—Ç:"""
                    
                    generated_with_rag = generate_with_lstm(
                        model, vocab, rag_prompt, 
                        max_len=max_gen_len, 
                        temperature=temperature, 
                        device=device
                    )
                    
                    answer_with_rag = extract_answer_from_generated(generated_with_rag, question)
                    predictions_with_rag.append(answer_with_rag)
                else:
                    predictions_with_rag.append("–ù–µ –Ω–∞–π–¥–µ–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –ö–æ–ê–ü –†–§.")
                    
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å RAG: {e}")
                predictions_with_rag.append("–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏")
        else:
            predictions_with_rag.append("RAG –Ω–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª—Å—è")
        
        try:
            generated_without_rag = generate_with_lstm(
                model, vocab, base_prompt, 
                max_len=max_gen_len, 
                temperature=temperature, 
                device=device
            )
            
            answer_without_rag = extract_answer_from_generated(generated_without_rag, question)
            predictions_without_rag.append(answer_without_rag)
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –±–µ–∑ RAG: {e}")
            predictions_without_rag.append("–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏")
    
    print("\n" + "="*70)
    print("–í–´–ß–ò–°–õ–ï–ù–ò–ï –ú–ï–¢–†–ò–ö")
    print("="*70)
    
    results = {
        "model_path": model_path,
        "use_rag": use_rag,
        "test_samples": len(test_data),
        "max_gen_len": max_gen_len,
        "temperature": temperature,
        "questions": questions,
        "references": references
    }
    
    if use_rag:
        print("\nüìä –ú–ï–¢–†–ò–ö–ò LSTM –° –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–ï–ú RAG:")
        print("-" * 40)
        
        rouge_scores_rag = calculate_rouge(predictions_with_rag, references)
        print("ROUGE –º–µ—Ç—Ä–∏–∫–∏:")
        for key, value in rouge_scores_rag.items():
            if isinstance(value, float):
                print(f"  {key}: {value:.4f}")
        
        bleu_scores_rag = calculate_bleu(predictions_with_rag, references)
        print(f"\nBLEU –º–µ—Ç—Ä–∏–∫–∞: {bleu_scores_rag['bleu']:.4f}")
        
        exact_match_rag = calculate_exact_match(predictions_with_rag, references)
        print(f"Exact Match: {exact_match_rag['exact_match']:.4f}")
        
        results["metrics_with_rag"] = {
            "rouge": rouge_scores_rag,
            "bleu": bleu_scores_rag,
            "exact_match": exact_match_rag,
            "predictions": predictions_with_rag
        }
    
    print("\nüìä –ú–ï–¢–†–ò–ö–ò LSTM –ë–ï–ó RAG:")
    print("-" * 40)
    
    rouge_scores_no_rag = calculate_rouge(predictions_without_rag, references)
    print("ROUGE –º–µ—Ç—Ä–∏–∫–∏:")
    for key, value in rouge_scores_no_rag.items():
        if isinstance(value, float):
            print(f"  {key}: {value:.4f}")
    
    bleu_scores_no_rag = calculate_bleu(predictions_without_rag, references)
    print(f"\nBLEU –º–µ—Ç—Ä–∏–∫–∞: {bleu_scores_no_rag['bleu']:.4f}")
    
    exact_match_no_rag = calculate_exact_match(predictions_without_rag, references)
    print(f"Exact Match: {exact_match_no_rag['exact_match']:.4f}")
    
    results["metrics_without_rag"] = {
        "rouge": rouge_scores_no_rag,
        "bleu": bleu_scores_no_rag,
        "exact_match": exact_match_no_rag,
        "predictions": predictions_without_rag
    }
    
    print("\n" + "="*70)
    print("–°–†–ê–í–ù–ò–¢–ï–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó LSTM")
    print("="*70)
    
    if use_rag:
        print("\nüìà –£–õ–£–ß–®–ï–ù–ò–ï/–£–•–£–î–®–ï–ù–ò–ï –ü–†–ò –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–ò RAG:")
        print("-" * 40)
        
        metrics_to_compare = ['rouge1', 'rouge2', 'rougeL', 'bleu', 'exact_match']
        for metric in metrics_to_compare:
            if metric == 'bleu':
                rag_val = bleu_scores_rag['bleu']
                no_rag_val = bleu_scores_no_rag['bleu']
            elif metric == 'exact_match':
                rag_val = exact_match_rag['exact_match']
                no_rag_val = exact_match_no_rag['exact_match']
            else:
                rag_val = rouge_scores_rag[metric]
                no_rag_val = rouge_scores_no_rag[metric]
            
            improvement = ((rag_val - no_rag_val) / no_rag_val * 100) if no_rag_val > 0 else 0
            trend = "üîº –£–õ–£–ß–®–ï–ù–ò–ï" if improvement > 0 else "üîΩ –£–•–£–î–®–ï–ù–ò–ï"
            
            print(f"{metric.upper():12} | –ë–µ–∑ RAG: {no_rag_val:.4f} | –° RAG: {rag_val:.4f} | {trend}: {improvement:+.2f}%")
    
    print("\nüìè –ê–ù–ê–õ–ò–ó –î–õ–ò–ù –û–¢–í–ï–¢–û–í LSTM:")
    print("-" * 40)
    
    if use_rag:
        avg_len_rag = sum(len(p.split()) for p in predictions_with_rag if isinstance(p, str)) / len(predictions_with_rag)
        print(f"–°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ –æ—Ç–≤–µ—Ç–∞ —Å RAG: {avg_len_rag:.1f} —Å–ª–æ–≤")
    
    avg_len_no_rag = sum(len(p.split()) for p in predictions_without_rag if isinstance(p, str)) / len(predictions_without_rag)
    print(f"–°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ –æ—Ç–≤–µ—Ç–∞ –±–µ–∑ RAG: {avg_len_no_rag:.1f} —Å–ª–æ–≤")
    
    avg_len_ref = sum(len(r.split()) for r in references) / len(references)
    print(f"–°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —ç—Ç–∞–ª–æ–Ω–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞: {avg_len_ref:.1f} —Å–ª–æ–≤")
    
    print("\nüîë –ü–†–û–í–ï–†–ö–ê –ö–õ–Æ–ß–ï–í–´–• –°–õ–û–í:")
    print("-" * 40)
    
    keywords = ["—Å—Ç–∞—Ç—å—è", "–ö–æ–ê–ü", "—à—Ç—Ä–∞—Ñ", "—Ä—É–±–ª–µ–π", "–∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–∏–≤–Ω"]
    
    print("–ë–µ–∑ RAG:")
    for keyword in keywords:
        count = sum(1 for p in predictions_without_rag if keyword.lower() in p.lower())
        percentage = (count / len(predictions_without_rag)) * 100
        print(f"  '{keyword}': {count}/{len(predictions_without_rag)} ({percentage:.1f}%)")
    
    if use_rag:
        print("\n–° RAG:")
        for keyword in keywords:
            count = sum(1 for p in predictions_with_rag if keyword.lower() in p.lower())
            percentage = (count / len(predictions_with_rag)) * 100
            print(f"  '{keyword}': {count}/{len(predictions_with_rag)} ({percentage:.1f}%)")
    
    print("\n" + "="*70)
    print("–ü–†–ò–ú–ï–†–´ –û–¢–í–ï–¢–û–í LSTM (–ø–µ—Ä–≤—ã–µ 3 –≤–æ–ø—Ä–æ—Å–∞)")
    print("="*70)
    
    for i in range(min(3, len(questions))):
        print(f"\n{i+1}. –í–û–ü–†–û–°: {questions[i]}")
        print(f"   –≠–¢–ê–õ–û–ù: {references[i]}")
        
        if use_rag:
            print(f"   LSTM –° RAG: {predictions_with_rag[i][:150]}...")
        
        print(f"   LSTM –ë–ï–ó RAG: {predictions_without_rag[i][:150]}...")
        print("-" * 50)
    
    timestamp = os.path.splitext(os.path.basename(model_path))[0]
    output_file = f"lstm_evaluation_{'with_rag' if use_rag else 'without_rag'}_{timestamp}.json"
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f"\n‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {output_file}")
    
    return results

def quick_test_lstm():
    print("üöÄ –ë–´–°–¢–†–´–ô –¢–ï–°–¢ LSTM –° RAG –°–ò–°–¢–ï–ú–û–ô")
    
    model, vocab, device = load_lstm_model()
    
    rag = MinimalRAG(use_cpu=True)
    
    if not rag.load_index("rag_index.pkl"):
        print("–ò–Ω–¥–µ–∫—Å –Ω–µ –Ω–∞–π–¥–µ–Ω. –ó–∞–ø—É—Å—Ç–∏—Ç–µ –ø–æ–ª–Ω—É—é –æ—Ü–µ–Ω–∫—É —Å–Ω–∞—á–∞–ª–∞.")
        return
    
    test_questions = [
        "–ö–∞–∫–∞—è —Å—Ç–∞—Ç—å—è –ö–æ–ê–ü –†–§ —Ä–µ–≥—É–ª–∏—Ä—É–µ—Ç –ø—Ä–µ–≤—ã—à–µ–Ω–∏–µ —Å–∫–æ—Ä–æ—Å—Ç–∏?",
        "–ö–∞–∫–æ–π —à—Ç—Ä–∞—Ñ –∑–∞ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–º –±–µ–∑ –ø—Ä–∞–≤?",
        "–ß—Ç–æ —Ç–∞–∫–æ–µ –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–∏–≤–Ω–æ–µ –ø—Ä–∞–≤–æ–Ω–∞—Ä—É—à–µ–Ω–∏–µ?"
    ]
    
    print("\n" + "="*60)
    for i, question in enumerate(test_questions, 1):
        print(f"\n{i}. –í–û–ü–†–û–°: {question}")
        print("-" * 40)
        
        context_chunks = rag.search(question, top_k=2)
        
        if context_chunks:
            print("–ù–∞–π–¥–µ–Ω–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –≤ RAG:")
            for j, chunk in enumerate(context_chunks, 1):
                article = chunk['info'].get('article', '–ù–µ —É–∫–∞–∑–∞–Ω–∞')[:50]
                print(f"  {j}. {article}...")
        
        if context_chunks:
            context = "\n".join([f"–§—Ä–∞–≥–º–µ–Ω—Ç {i+1}: {chunk['text'][:100]}..." 
                               for i, chunk in enumerate(context_chunks)])
            
            rag_prompt = f"""–ö–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –ö–æ–ê–ü –†–§:
{context}

–ù–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –æ—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å: {question}

–û—Ç–≤–µ—Ç:"""
            
            generated_with_rag = generate_with_lstm(model, vocab, rag_prompt, max_len=100, temperature=0.7, device=device)
            answer_with_rag = extract_answer_from_generated(generated_with_rag, question)
            print(f"\nLSTM –° RAG: {answer_with_rag}")
        
        base_prompt = f"–í–æ–ø—Ä–æ—Å: {question} –û—Ç–≤–µ—Ç:"
        generated_without_rag = generate_with_lstm(model, vocab, base_prompt, max_len=100, temperature=0.7, device=device)
        answer_without_rag = extract_answer_from_generated(generated_without_rag, question)
        print(f"\nLSTM –ë–ï–ó RAG: {answer_without_rag}")

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='–û—Ü–µ–Ω–∫–∞ LSTM –º–æ–¥–µ–ª–∏ —Å RAG —Å–∏—Å—Ç–µ–º–æ–π')
    parser.add_argument('--model_path', type=str, default="lstm_koap_model_best.pt",
                       help='–ü—É—Ç—å –∫ –º–æ–¥–µ–ª–∏ LSTM')
    parser.add_argument('--vocab_path', type=str, default="lstm_koap_model_vocab.pkl",
                       help='–ü—É—Ç—å –∫ —Å–ª–æ–≤–∞—Ä—é')
    parser.add_argument('--no_rag', action='store_true',
                       help='–û—Ü–µ–Ω–∏–≤–∞—Ç—å —Ç–æ–ª—å–∫–æ –±–µ–∑ RAG')
    parser.add_argument('--quick_test', action='store_true',
                       help='–ë—ã—Å—Ç—Ä—ã–π —Ç–µ—Å—Ç LSTM —Å RAG —Å–∏—Å—Ç–µ–º–æ–π')
    parser.add_argument('--rag_index', type=str, default="rag_index.pkl",
                       help='–ü—É—Ç—å –∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–º—É –∏–Ω–¥–µ–∫—Å—É RAG')
    parser.add_argument('--koap_data', type=str, default="data/koap_data",
                       help='–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –¥–∞–Ω–Ω—ã–º–∏ –ö–æ–ê–ü')
    parser.add_argument('--test_data', type=str, default="data/test_data.json",
                       help='–ü—É—Ç—å –∫ —Ç–µ—Å—Ç–æ–≤—ã–º –¥–∞–Ω–Ω—ã–º')
    parser.add_argument('--max_len', type=int, default=100,
                       help='–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º–æ–≥–æ —Ç–µ–∫—Å—Ç–∞')
    parser.add_argument('--temperature', type=float, default=0.7,
                       help='–¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏')
    
    args = parser.parse_args()
    
    if args.quick_test:
        quick_test_lstm()
    else:
        evaluate_lstm_with_rag(
            model_path=args.model_path,
            vocab_path=args.vocab_path,
            use_rag=not args.no_rag,
            rag_index_path=args.rag_index,
            koap_data_dir=args.koap_data,
            test_data_path=args.test_data,
            max_gen_len=args.max_len,
            temperature=args.temperature
        )